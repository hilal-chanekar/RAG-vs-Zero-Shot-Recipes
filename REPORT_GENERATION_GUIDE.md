# Experiment Report Generation Checklist

## Overview
This document outlines what data needs to be collected for generating a comprehensive experiment report similar to the reference report.

---

## âœ… Data Collection Requirements

### 1. **Experiment Metadata** (Auto-collected)
- [x] Experiment name
- [x] Condition (zero-shot, few-shot_RAG)
- [x] Model name and version
- [x] Temperature
- [x] Max tokens
- [x] Retrieval model (if RAG)
- [x] Number of retrieved items (k)
- [x] Start timestamp
- [x] End timestamp
- [x] Total duration (seconds)
- [x] Number of samples processed

**Collected via**: `evaluation/experiment_logger.py`  
**Output**: `results/metadata_[condition]_[timestamp].json`

---

### 2. **Generation Results** (Already Available)
- [x] Input prompts
- [x] Generated outputs
- [x] Retrieved recipes (for RAG)
- [x] Recipe names/IDs
- [x] Model configuration

**Current location**: 
- `results/zero_shot.json`
- `results/few_shot_RAG.json`

---

### 3. **Quality Metrics** (Auto-calculated)
- [x] Number of ingredients per recipe
- [x] Number of steps per recipe
- [x] JSON parsing errors
- [x] Valid vs invalid outputs

**Calculated via**: `evaluation/metrics_calculator.py`  
**Output**: `results/metrics_comparison.json`

---

### 4. **Retrieval Performance** (Optional for baseline)
For a more detailed report, consider adding:
- [ ] Similarity scores of retrieved recipes
- [ ] Top-k retrieval accuracy
- [ ] Recipe category matching

**Location**: `retrieval/recipe_retriever.py` (can extend if needed)

---

### 5. **Evaluation Metrics** (Optional - Requires additional scripts)
Advanced metrics for a richer report:
- [ ] Ingredient overlap between generated and retrieved recipes
- [ ] Novel ingredients introduced (hallucinations)
- [ ] Semantic similarity to base recipe
- [ ] Constraint violations (if task-specific)

**Future**: Create `evaluation/semantic_evaluator.py` if needed

---

## ğŸ“‹ Report Structure (How data will be used)

```
EXPERIMENT REPORT
â”œâ”€â”€ Executive Summary
â”‚   â”œâ”€â”€ Key findings from metrics_comparison.json
â”‚   â”œâ”€â”€ Duration from metadata JSON
â”‚   â””â”€â”€ Sample count
â”œâ”€â”€ Research Question & Hypothesis
â”‚   â””â”€â”€ Auto-inserted from template
â”œâ”€â”€ Experiment Design
â”‚   â”œâ”€â”€ Task description
â”‚   â”œâ”€â”€ Conditions (from metadata)
â”‚   â”œâ”€â”€ Controlled variables
â”‚   â””â”€â”€ Dataset info (from recipes.json)
â”œâ”€â”€ Dataset
â”‚   â”œâ”€â”€ Source: recipes.json
â”‚   â”œâ”€â”€ Total recipes
â”‚   â”œâ”€â”€ Categories
â”‚   â””â”€â”€ Filtering criteria
â”œâ”€â”€ Retrieval Method
â”‚   â”œâ”€â”€ Model: gte-large-en-v1.5 (from metadata)
â”‚   â”œâ”€â”€ k value (from metadata)
â”‚   â””â”€â”€ Similarity metric
â”œâ”€â”€ Evaluation Metrics
â”‚   â””â”€â”€ Quality metrics (from metrics_comparison.json)
â”œâ”€â”€ Results
â”‚   â”œâ”€â”€ Summary statistics
â”‚   â”œâ”€â”€ Comparison table (zero-shot vs RAG)
â”‚   â”œâ”€â”€ Key findings
â”‚   â””â”€â”€ Charts/visualizations (optional)
â”œâ”€â”€ Experiment Details
â”‚   â”œâ”€â”€ Runtime (from metadata)
â”‚   â”œâ”€â”€ Model configuration
â”‚   â”œâ”€â”€ Completion date
â”‚   â””â”€â”€ Number of LLM calls
â”œâ”€â”€ Limitations
â”‚   â””â”€â”€ Auto-inserted
â”œâ”€â”€ Next Steps
â”‚   â””â”€â”€ Auto-inserted
â”œâ”€â”€ Reproducibility
â”‚   â”œâ”€â”€ Model details (from metadata)
â”‚   â”œâ”€â”€ Code locations
â”‚   â”œâ”€â”€ Random seed info
â”‚   â””â”€â”€ Results files
â””â”€â”€ Appendices
    â”œâ”€â”€ Prompt templates (from generation/*.py)
    â””â”€â”€ Sample outputs (from results/*.json)
```

---

## ğŸš€ How to Run Experiments

### Step 1: Run both conditions with logging
```bash
cd /Users/hilal/Documents/Python Projects/NLP/RAG-vs-Zero-Shot-Recipes
source .venv/bin/activate
PYTHONPATH="$PWD" python evaluation/run_experiments.py --condition both --compare
```

This will:
- âœ… Run zero-shot generation
- âœ… Run few-shot RAG generation
- âœ… Collect timing metadata
- âœ… Save metadata JSONs
- âœ… Calculate comparison metrics
- âœ… Save metrics report

### Step 2: Verify outputs
```
results/
â”œâ”€â”€ zero_shot.json              â† Generation results
â”œâ”€â”€ few_shot_RAG.json           â† Generation results
â”œâ”€â”€ metadata_zero_shot_[ts].json â† Timing & config
â”œâ”€â”€ metadata_few_shot_RAG_[ts].json â† Timing & config
â””â”€â”€ metrics_comparison.json     â† Quality metrics
```

### Step 3: Generate report
Once experiments complete, prompt:
```
"Generate the experiment report using the data from results/ folder"
```

---

## ğŸ“Š Data Available NOW

### Already in repo:
- âœ… Prompt templates (generation/*.py)
- âœ… Model configuration
- âœ… Recipe dataset (recipes.json, ~1000 recipes)
- âœ… Retrieval method (recipe_retriever.py)
- âœ… Generation results (results/*.json)

### Will be created after experiments:
- â³ Timing metadata
- â³ Quality metrics
- â³ Comparison analysis

---

## ğŸ¯ Next Steps

1. **Run the experiments** using the command above
2. **Verify all output files** exist in `results/`
3. **Prompt to generate report** with all collected data

The system is now ready to generate a comprehensive report after you run the experiments!
